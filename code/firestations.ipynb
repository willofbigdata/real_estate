{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 消防局資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 功能:\n",
    "# 爬出消防局資料\n",
    "\n",
    "# 爬出屬性:\n",
    "# 消防局名稱\n",
    "# 地址\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Downloads/路外停車資訊.json') as data_json:\n",
    "    data = json.load(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each in data['parkingLots']:\n",
    "    print(each['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 步驟1\n",
    "# 引用所有會用到的套件\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import bs4\n",
    "import time\n",
    "import re\n",
    "import sqlite3\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 台北市政府消防局\n",
    "# 取得各救護大隊基本資訊頁面的url\n",
    "\n",
    "url = \"http://www.119.gov.taipei/detail.php?type=article&id=11993\"\n",
    "\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.select(a[id=\"12033\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 步驟2\n",
    "# 定義 job104_for_info 方法\n",
    "# 參數href為要爬的網頁的網址\n",
    "# 爬網頁之後解析出內文並回存進資料庫\n",
    "# (假設所需的套件都已經啟用)\n",
    "\n",
    "# 請依需求更改p1為p2或p3\n",
    "\n",
    "def job104_for_info(href): \n",
    "\n",
    "    try:\n",
    "\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # 爬進資料\n",
    "        # 用lxml會出現錯誤,得用Python預設的html.parser\n",
    "        \n",
    "        res = requests.get(href)\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "        \n",
    "        if soup.select('head > title') != \"104人力銀行─錯誤頁\":    # 跳過過期的職缺頁面\n",
    "\n",
    "            # 拆解資料\n",
    "\n",
    "            # 工作內容 (job_desc)\n",
    "\n",
    "            job_desc = soup.select('div[class=\"content\"] > p')[0].text\n",
    "\n",
    "            # 擅長工具 (job_tools)\n",
    "            # 工作技能 (job_skills)\n",
    "            # 其他條件 (other_cons)\n",
    "\n",
    "            reqs = soup.find_all([\"dt\",\"dd\"])\n",
    "\n",
    "            job_tools = \"\"\n",
    "            job_skills = \"\"\n",
    "            other_cons = \"\"\n",
    "\n",
    "            for i in range(0,len(reqs)-1):\n",
    "                if \"擅長工具\" in reqs[i].text:\n",
    "                    job_tools += reqs[i+1].text\n",
    "                elif \"工作技能\" in reqs[i].text:\n",
    "                    job_skills += reqs[i+1].text\n",
    "                elif \"其他條件\" in reqs[i].text:\n",
    "                    other_cons += reqs[i+1].text\n",
    "\n",
    "            # 將找到的資料都串起來        \n",
    "\n",
    "            new_info = job_desc + \",\" + job_tools + \",\" + job_skills + \",\" + other_cons\n",
    "\n",
    "            for char in ['\\n','\\r','、','，']:\n",
    "                if char in new_info:\n",
    "                    new_info = new_info.replace(char,' ')\n",
    "\n",
    "            new_info = new_info.encode('ascii','ignore').decode('utf8')\n",
    "            # print(new_info)\n",
    "\n",
    "            # 將資料上傳到資料庫裡\n",
    "\n",
    "            with sqlite3.connect('../data/job104p1.sqlite') as conn:\n",
    "#             with sqlite3.connect('../data/job104p2.sqlite') as conn:\n",
    "#             with sqlite3.connect('../data/job104p3.sqlite') as conn:\n",
    "                c = conn.cursor()\n",
    "                save_info = \"update job104 set info = ? where href = ?;\" \n",
    "                c.execute(save_info,(new_info,href))\n",
    "        \n",
    "        else:\n",
    "            print(\"404網頁不存在\")\n",
    "    \n",
    "    #如果網頁異常則拋出例外\n",
    "    \n",
    "    except IndexError as e:\n",
    "        print(e, href)\n",
    "\n",
    "    except:\n",
    "        print(\"其他例外: \" + href)\n",
    "        \n",
    "#爬蟲例外 例外方法寫在def裏面\n",
    "        \n",
    "class CrawlerError(Exception): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 步驟3\n",
    "# 定義update_None_info方法\n",
    "# 從資料庫找出需要抓的網頁的網址\n",
    "# 然後一筆一筆抓\n",
    "# 存進資料庫\n",
    "\n",
    "# 請依需求更改p1為p2或p3\n",
    "\n",
    "def update_None_info(): \n",
    "       \n",
    "#     from bs4 import BeautifulSoup\n",
    "#     import requests\n",
    "#     import bs4\n",
    "#     import time\n",
    "#     import re\n",
    "#     import sqlite3\n",
    "    \n",
    "    try:\n",
    "        with sqlite3.connect('../data/job104p1.sqlite') as conn:\n",
    "#         with sqlite3.connect('../data/job104p2.sqlite') as conn:\n",
    "#         with sqlite3.connect('../data/job104p3.sqlite') as conn:\n",
    "\n",
    "            c = conn.cursor()\n",
    "            \n",
    "            # 找出info是空的url並找出網址list\n",
    "            \n",
    "            qryString = \"SELECT href FROM job104 where info is '';\" \n",
    "            c.execute(qryString)\n",
    "            \n",
    "            # 計算有多少筆完成\n",
    "            \n",
    "            do_number = 0 \n",
    "            lst = c.fetchall()\n",
    "            \n",
    "            # 控制爬網最大比數\n",
    "            \n",
    "#             lst[0:1000]    \n",
    "            \n",
    "            #跑迴圈執行\n",
    "            \n",
    "            for a in lst:\n",
    "                if \"hunter.104.com.tw\" not in a[0]: #開頭網址不是獵頭網的才進入迴圈\n",
    "                    job104_for_info(a[0]) \n",
    "#                     print(a[0])\n",
    "                    do_number +=1\n",
    "                    print(do_number)\n",
    "\n",
    "                else:\n",
    "                    print(\"獵頭網頁\") #若出現則顯示外包網頁\n",
    "                              \n",
    "            print('Has crawled {} info!'.format(do_number))\n",
    "                        \n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "        print(a)\n",
    "        print(href)\n",
    "        \n",
    "    except CrawlerError as e:\n",
    "        print(a)\n",
    "        print(e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 步驟3\n",
    "# 執行update_None_info方法\n",
    "\n",
    "update_None_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 步驟4\n",
    "# 檢視存進資料庫的資料\n",
    "\n",
    "# 請依需求更改p1為p2或p3\n",
    "\n",
    "with sqlite3.connect('../data/job104p1.sqlite') as conn: \n",
    "# with sqlite3.connect('../data/job104p2.sqlite') as conn: \n",
    "# with sqlite3.connect('../data/job104p3.sqlite') as conn: \n",
    "    c = conn.cursor()\n",
    "    pprint.pprint(list(c.execute('select * from job104 limit 0, 20;')))\n",
    "#     pprint.pprint(list(c.execute('select count(*) from job104 where info != \"\";')))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加入單筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jid = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with sqlite3.connect('../data/job.sqlite') as conn:\n",
    "#     c = conn.cursor()\n",
    "# #     c.execute('select * from job518')\n",
    "# #     print(c.fetchone())\n",
    "#     updStrting = 'update job518 set info =:info where jobID =:jobID'\n",
    "#     c.execute(updStrting, {'info' : new_info, 'jobID' : jid})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with sqlite3.connect('../data/job.sqlite') as conn:\n",
    "#     c = conn.cursor()\n",
    "#     c.execute('select * from job518')\n",
    "#     print(c.fetchone())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
